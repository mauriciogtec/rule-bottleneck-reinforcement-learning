# Description: PPO configuration file

# Rulebot arguments
embedder_lm: "togethercomputer/m2-bert-80M-8k-retrieval"
# the embedder model to use
embed_dim: 768
# the dimension of the mbeddings model
# chat_lm: "meta-llama/Llama-3.2-3B-Instruct-Turbo"
chat_lm: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
# chat_lm: "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
# the chat model to use
parallel_envs: false  # no need for this since the bottleneck is the LLM calls, not in the env steps
parallel_pipeline: true  # parallelize the LLM calls
# if toggled, the model will be trained in parallel
hidden_dim: 16
# the hidden dimension of the attention model projections
num_rules: 10
# the number of rules to use
resume: true
# if toggled, the model will be resumed from the last checkpoint
max_rule_combinations: 1
# the maximum number of rule combinations to use
exp_name: 
# the name of this experiment
seed: 1
# seed of the experiment
torch_deterministic: true
# if toggled, `torch.backends.cudnn.deterministic=False`
cuda: true

# Logging and tracking arguments
# if toggled, cuda will be enabled by default
track: false
# if toggled, this experiment will be tracked with Weights and Biases
wandb_project_name: "rulebots"
# the wandb's project name
wandb_entity: null
# the entity (team) of wandb's project
wandb_save_code: true
# if toggled, the code will be saved to wandb
capture_video: false
# whether to capture videos of the agent performances (check out `videos` folder)
log_examples_interval: 20
# the interval to log examples
save_interval: 1
# the interval to save the model

# PPO specific arguments
env_id: "Uganda"
# the id of the environment
total_timesteps: 10000  # 50x reduction
# total timesteps of the experiments
learning_rate: 2.5e-4
# the learning rate of the optimizer
num_envs: 8
# the number of parallel game environments
num_steps: 16  # 128
# the number of steps to run in each environment per policy rollout
anneal_lr: true # true
# Toggle learning rate annealing for policy and value networks
gamma: 0.99
# the discount factor gamma
gae_lambda: 0.95
# the lambda for the general advantage estimation
num_minibatches: 16 
# the number of mini-batches
update_epochs: 20  # 4
# the K epochs to update the policy
norm_adv: true
# Toggles advantages normalization
clip_coef: 0.2
# the surrogate clipping coefficient
clip_vloss: true
# Toggles whether or not to use a clipped loss for the value function, as per the paper.
ent_coef: 0.01
# coefficient of the entropy
vf_coef: 0.5
# coefficient of the value function
max_grad_norm: 0.5
# the maximum norm for the gradient clipping
target_kl: null
# the target KL divergence threshold

num_eval_steps: 64
# """the number of steps to run in each eval environment per policy rollout"""
eval_interval: 4
# """the evaluation interval"""
eval_deterministic: 1
# """if toggled, the evaluation will be deterministic"""
