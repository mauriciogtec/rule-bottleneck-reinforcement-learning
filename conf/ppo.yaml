embedder_lm: "togethercomputer/m2-bert-80M-8k-retrieval"
embed_dim: 768
# the embeddings model to use

# chat_lm: "meta-llama/Llama-3.2-3B-Instruct-Turbo"
chat_lm: "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
# the chat model to use

parallel: true
# if toggled, the model will be trained in parallel

hidden_dim: 16
# the hidden dimension of the attention model projections

num_rules: 10
# the number of rules to use

resume: false
# if toggled, the model will be resumed from the last checkpoint

max_rule_combinations: 1
# the maximum number of rule combinations to use

exp_name: 
# the name of this experiment
seed: 1
# seed of the experiment
torch_deterministic: true
# if toggled, `torch.backends.cudnn.deterministic=False`
cuda: true
# if toggled, cuda will be enabled by default
track: false
# if toggled, this experiment will be tracked with Weights and Biases
wandb_project_name: "rulebots"
# the wandb's project name
wandb_entity: null
# the entity (team) of wandb's project
wandb_save_code: true
# if toggled, the code will be saved to wandb
capture_video: false
# whether to capture videos of the agent performances (check out `videos` folder)
log_examples_interval: 20

# Algorithm specific arguments
env_id: "BuySell"
# the id of the environment
total_timesteps: 10000  # 50x reduction
# total timesteps of the experiments
learning_rate: 2.5e-4
# the learning rate of the optimizer
num_envs: 4
# the number of parallel game environments
num_steps: 16  # 128
# the number of steps to run in each environment per policy rollout
anneal_lr: true # true
# Toggle learning rate annealing for policy and value networks
gamma: 0.99
# the discount factor gamma
gae_lambda: 0.95
# the lambda for the general advantage estimation
num_minibatches: 4 
# the number of mini-batches
update_epochs: 20  # 4
# the K epochs to update the policy
norm_adv: true
# Toggles advantages normalization
clip_coef: 0.2
# the surrogate clipping coefficient
clip_vloss: true
# Toggles whether or not to use a clipped loss for the value function, as per the paper.
ent_coef: 0.01
# coefficient of the entropy
vf_coef: 0.5
# coefficient of the value function
max_grad_norm: 0.5
# the maximum norm for the gradient clipping
target_kl: null
# the target KL divergence threshold
