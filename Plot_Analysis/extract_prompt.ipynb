{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File /Users/haichuan/Desktop/RL_Explanation_Experiment/rulebots/Plot_Analysis/artifacts/run-gzs3h37r-history:v0/wandb-history.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the history file (typically a JSON or CSV)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m history_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifact_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/wandb-history.json\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Or wandb-history.csv based on format\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m history_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(history_file, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Use read_csv() if it's a CSV\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Inspect the data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(history_data\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m JsonReader(\n\u001b[1;32m    792\u001b[0m     path_or_buf,\n\u001b[1;32m    793\u001b[0m     orient\u001b[38;5;241m=\u001b[39morient,\n\u001b[1;32m    794\u001b[0m     typ\u001b[38;5;241m=\u001b[39mtyp,\n\u001b[1;32m    795\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    796\u001b[0m     convert_axes\u001b[38;5;241m=\u001b[39mconvert_axes,\n\u001b[1;32m    797\u001b[0m     convert_dates\u001b[38;5;241m=\u001b[39mconvert_dates,\n\u001b[1;32m    798\u001b[0m     keep_default_dates\u001b[38;5;241m=\u001b[39mkeep_default_dates,\n\u001b[1;32m    799\u001b[0m     precise_float\u001b[38;5;241m=\u001b[39mprecise_float,\n\u001b[1;32m    800\u001b[0m     date_unit\u001b[38;5;241m=\u001b[39mdate_unit,\n\u001b[1;32m    801\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    802\u001b[0m     lines\u001b[38;5;241m=\u001b[39mlines,\n\u001b[1;32m    803\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    804\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    805\u001b[0m     nrows\u001b[38;5;241m=\u001b[39mnrows,\n\u001b[1;32m    806\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    807\u001b[0m     encoding_errors\u001b[38;5;241m=\u001b[39mencoding_errors,\n\u001b[1;32m    808\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    809\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m    810\u001b[0m )\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File /Users/haichuan/Desktop/RL_Explanation_Experiment/rulebots/Plot_Analysis/artifacts/run-gzs3h37r-history:v0/wandb-history.json does not exist"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Fetch the artifact\n",
    "artifact = api.artifact('rulebots/rulebots/run-gzs3h37r-history:v0', type='wandb-history')  # Replace <id> with the actual run ID\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load the history file (typically a JSON or CSV)\n",
    "history_file = f\"{artifact_dir}/wandb-history.json\"  # Or wandb-history.csv based on format\n",
    "history_data = pd.read_json(history_file, lines=True)  # Use read_csv() if it's a CSV\n",
    "\n",
    "# Inspect the data\n",
    "print(history_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: /Users/haichuan/Desktop/RL_Explanation_Experiment/rulebots/Plot_Analysis/artifacts/run-gzs3h37r-history:v0\n",
      "File: 0000.parquet\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "# Initialize the API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Fetch the artifact\n",
    "artifact = api.artifact('rulebots/rulebots/run-gzs3h37r-history:v0', type='wandb-history')  # Replace <id> with the actual run ID\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# List all files and directories in the artifact\n",
    "for root, dirs, files in os.walk(artifact_dir):\n",
    "    print(f\"Directory: {root}\")\n",
    "    for file in files:\n",
    "        print(f\"File: {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   losses/explained_variance    _timestamp  charts/SPS  charts/learning_rate  \\\n",
      "0                        NaN  1.737670e+09         NaN                   NaN   \n",
      "1                        NaN  1.737670e+09         NaN                   NaN   \n",
      "2                        NaN  1.737670e+09         NaN                   NaN   \n",
      "3                        NaN  1.737670e+09         NaN                   NaN   \n",
      "4                        NaN  1.737670e+09         NaN                   NaN   \n",
      "\n",
      "   charts/episodic_length  losses/approx_kl   _runtime  global_step  \\\n",
      "0                     NaN               NaN  16.152299        256.0   \n",
      "1                    64.0               NaN  16.152352        256.0   \n",
      "2                     NaN               NaN  16.152381        256.0   \n",
      "3                    64.0               NaN  16.152402        256.0   \n",
      "4                     NaN               NaN  16.152429        256.0   \n",
      "\n",
      "   losses/policy_loss  charts/episodic_return  losses/old_approx_kl  \\\n",
      "0                 NaN              -14.696245                   NaN   \n",
      "1                 NaN                     NaN                   NaN   \n",
      "2                 NaN              -26.233347                   NaN   \n",
      "3                 NaN                     NaN                   NaN   \n",
      "4                 NaN              -22.901787                   NaN   \n",
      "\n",
      "   losses/clipfrac  losses/value_loss  _step  losses/entropy  \n",
      "0              NaN                NaN    0.0             NaN  \n",
      "1              NaN                NaN    1.0             NaN  \n",
      "2              NaN                NaN    2.0             NaN  \n",
      "3              NaN                NaN    3.0             NaN  \n",
      "4              NaN                NaN    4.0             NaN  \n",
      "Index(['losses/explained_variance', '_timestamp', 'charts/SPS',\n",
      "       'charts/learning_rate', 'charts/episodic_length', 'losses/approx_kl',\n",
      "       '_runtime', 'global_step', 'losses/policy_loss',\n",
      "       'charts/episodic_return', 'losses/old_approx_kl', 'losses/clipfrac',\n",
      "       'losses/value_loss', '_step', 'losses/entropy'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"artifacts/run-gzs3h37r-history:v0/0000.parquet\"\n",
    "\n",
    "# Load the Parquet file into a pandas DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Display the first few rows to inspect the content\n",
    "print(df.head())\n",
    "\n",
    "# Check the columns to understand the structure\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in workspace:\n",
      "artifact/1460720264/wandb_manifest.json\n",
      "code/sac_attention.py\n",
      "diff.patch\n",
      "requirements.txt\n",
      "wandb-metadata.json\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize the API\n",
    "api = wandb.Api()\n",
    "entity = \"rulebots\"\n",
    "project = \"rulebots\"\n",
    "# Access the specific run (replace <entity>, <project>, and <run_id>)\n",
    "runs = api.runs(f\"{entity}/{project}\")  # Replace with your specific run ID\n",
    "\n",
    "\n",
    "# List all files in the workspace\n",
    "run = runs[0]\n",
    "files = run.files()\n",
    "print(\"Files in workspace:\")\n",
    "for file in files:\n",
    "    print(file.name)\n",
    "\n",
    "    # Download specific file if found\n",
    "    # for file in files:\n",
    "    #     if \"text_logs\" in file.name or \"jsonl\" in file.name:  # Adjust based on expected file name\n",
    "    #         print(f\"Downloading: {file.name}\")\n",
    "    #         file.download(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of wandb_manifest.json:\n",
      "{\n",
      "    \"version\": 1,\n",
      "    \"storagePolicy\": \"wandb-storage-policy-v1\",\n",
      "    \"storagePolicyConfig\": {\n",
      "        \"storageLayout\": \"V2\"\n",
      "    },\n",
      "    \"contents\": {\n",
      "        \"0000.parquet\": {\n",
      "            \"digest\": \"6n8RbkHrB7ntdq4tkRb+sw==\",\n",
      "            \"birthArtifactID\": \"QXJ0aWZhY3Q6MTQ2MDcyMDI2NA==\",\n",
      "            \"size\": 212923\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# File paths\n",
    "manifest_path = \"artifact/1460720264/wandb_manifest.json\"\n",
    "metadata_path = \"wandb-metadata.json\"\n",
    "\n",
    "# Load and print the contents of the wandb_manifest.json\n",
    "with open(manifest_path, \"r\") as manifest_file:\n",
    "    manifest_data = json.load(manifest_file)\n",
    "    print(\"Contents of wandb_manifest.json:\")\n",
    "    print(json.dumps(manifest_data, indent=4))  # Pretty-print JSON\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contents of wandb-metadata.json:\n",
      "{\n",
      "    \"os\": \"Linux-4.18.0-513.18.1.el8_9.x86_64-x86_64-with-glibc2.28\",\n",
      "    \"python\": \"CPython 3.10.16\",\n",
      "    \"startedAt\": \"2025-01-22T21:18:59.953699Z\",\n",
      "    \"args\": [\n",
      "        \"--wandb-entity\",\n",
      "        \"rulebots\",\n",
      "        \"--track\",\n",
      "        \"--seed\",\n",
      "        \"123\"\n",
      "    ],\n",
      "    \"program\": \"/n/home06/haichuan/rulebots/sac_attention.py\",\n",
      "    \"codePath\": \"sac_attention.py\",\n",
      "    \"git\": {\n",
      "        \"remote\": \"https://github.com/mauriciogtec/rulebots.git\",\n",
      "        \"commit\": \"366292aa0647f160ce69bd15220d07394fff5136\"\n",
      "    },\n",
      "    \"email\": \"haichuanwang23@gmail.com\",\n",
      "    \"root\": \"/n/home06/haichuan/rulebots\",\n",
      "    \"host\": \"holy7c04104.rc.fas.harvard.edu\",\n",
      "    \"executable\": \"/n/home06/haichuan/.conda/envs/rulebots/bin/python\",\n",
      "    \"codePathLocal\": \"sac_attention.py\",\n",
      "    \"cpu_count\": 48,\n",
      "    \"cpu_count_logical\": 48,\n",
      "    \"disk\": {\n",
      "        \"/\": {\n",
      "            \"total\": \"53660876800\",\n",
      "            \"used\": \"10906275840\"\n",
      "        }\n",
      "    },\n",
      "    \"memory\": {\n",
      "        \"total\": \"201927192576\"\n",
      "    },\n",
      "    \"cpu\": {\n",
      "        \"count\": 48,\n",
      "        \"countLogical\": 48\n",
      "    },\n",
      "    \"slurm\": {\n",
      "        \"array_job_id\": \"382931\",\n",
      "        \"array_task_count\": \"3\",\n",
      "        \"array_task_id\": \"1\",\n",
      "        \"array_task_max\": \"2\",\n",
      "        \"array_task_min\": \"0\",\n",
      "        \"array_task_step\": \"1\",\n",
      "        \"cluster_name\": \"odyssey\",\n",
      "        \"conf\": \"/etc/slurm/slurm.conf\",\n",
      "        \"cpus_on_node\": \"8\",\n",
      "        \"gtids\": \"0\",\n",
      "        \"job_account\": \"tambe_lab\",\n",
      "        \"job_cpus_per_node\": \"8\",\n",
      "        \"job_end_time\": \"1737623855\",\n",
      "        \"job_gid\": \"5100\",\n",
      "        \"job_id\": \"388278\",\n",
      "        \"job_name\": \"cpu_sac_batch.slurm\",\n",
      "        \"job_nodelist\": \"holy7c04104\",\n",
      "        \"job_num_nodes\": \"1\",\n",
      "        \"job_partition\": \"shared\",\n",
      "        \"job_qos\": \"normal\",\n",
      "        \"job_start_time\": \"1737580655\",\n",
      "        \"job_uid\": \"66195\",\n",
      "        \"job_user\": \"haichuan\",\n",
      "        \"jobid\": \"388278\",\n",
      "        \"localid\": \"0\",\n",
      "        \"mem_per_node\": \"12288\",\n",
      "        \"nnodes\": \"1\",\n",
      "        \"nodeid\": \"0\",\n",
      "        \"nodelist\": \"holy7c04104\",\n",
      "        \"nprocs\": \"8\",\n",
      "        \"ntasks\": \"8\",\n",
      "        \"prio_process\": \"0\",\n",
      "        \"procid\": \"0\",\n",
      "        \"script_context\": \"prolog_task\",\n",
      "        \"stepmgr\": \"holy7c04104\",\n",
      "        \"submit_dir\": \"/n/home06/haichuan/rulebots\",\n",
      "        \"submit_host\": \"boslogin06.rc.fas.harvard.edu\",\n",
      "        \"task_pid\": \"2082509\",\n",
      "        \"tasks_per_node\": \"8\",\n",
      "        \"topology_addr\": \"main.holyoke.holyhdr.holy7c04104\",\n",
      "        \"topology_addr_pattern\": \"switch.switch.switch.node\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load and print the contents of the wandb-metadata.json\n",
    "with open(metadata_path, \"r\") as metadata_file:\n",
    "    metadata_data = json.load(metadata_file)\n",
    "    print(\"\\nContents of wandb-metadata.json:\")\n",
    "    print(json.dumps(metadata_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in workspace:\n",
      "Downloading: artifact/1460720264/wandb_manifest.json\n",
      "Downloading: wandb-metadata.json\n"
     ]
    }
   ],
   "source": [
    "# List all files in the workspace\n",
    "files = run.files()\n",
    "print(\"Files in workspace:\")\n",
    "\n",
    "# Download the specific files\n",
    "for file in files:\n",
    "    if file.name == \"artifact/1460720264/wandb_manifest.json\" or file.name == \"wandb-metadata.json\":\n",
    "        print(f\"Downloading: {file.name}\")\n",
    "        file.download(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files for run sac_attention_Uganda__sac_attention__gpt-4o-mini-huit__123__1737580727:\n",
      "Files for run sac_attention_Uganda__sac_attention__gpt-4o-mini-huit__456__1737580727:\n",
      "Files for run sac_attention_Uganda__sac_attention__gpt-4o-mini-huit__42__1737580730:\n",
      "Files for run eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__42__1737583030:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__42__1737583030/events.out.tfevents.1737583044.holy7c08309.rc.fas.harvard.edu.1298129.0\n",
      "Files for run eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__eval_llm__42__1737589480:\n",
      "Files for run eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__42__1737604403:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__42__1737604403/events.out.tfevents.1737604411.holy7c04104.rc.fas.harvard.edu.2153643.0\n",
      "Files for run eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__456__1737604442:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__456__1737604442/events.out.tfevents.1737604455.holy7c04109.rc.fas.harvard.edu.1110254.0\n",
      "Files for run eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__123__1737604448:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__123__1737604448/events.out.tfevents.1737604464.holy7c04106.rc.fas.harvard.edu.1005930.0\n",
      "Files for run eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__eval_llm__123__1737605000:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__eval_llm__123__1737605000/events.out.tfevents.1737605014.holy7c04202.rc.fas.harvard.edu.3096462.0\n",
      "Files for run eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__eval_llm__456__1737605001:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__eval_llm__456__1737605001/events.out.tfevents.1737605015.holy7c04204.rc.fas.harvard.edu.3116655.0\n",
      "Files for run eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__eval_llm__42__1737605005:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__eval_llm__42__1737605005/events.out.tfevents.1737605020.holy7c04111.rc.fas.harvard.edu.2687406.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__eval_llm__42__1737605230:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__eval_llm__42__1737605230/events.out.tfevents.1737605245.holy7c04205.rc.fas.harvard.edu.3327107.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__eval_llm__456__1737605230:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__eval_llm__456__1737605230/events.out.tfevents.1737605244.holy7c04210.rc.fas.harvard.edu.4015940.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__eval_llm__123__1737605230:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__eval_llm__123__1737605230/events.out.tfevents.1737605245.holy7c04205.rc.fas.harvard.edu.3327106.0\n",
      "Files for run random_policy_eval__Uganda__64__1737662682:\n",
      "TensorBoard log found: runs/random_policy_eval__Uganda__64__1737662682/events.out.tfevents.1737662684.HSPH-Mauricios-MacBook-Pro.7857.0\n",
      "Files for run numeric_ppo_eval__UgandaNumeric__64__1737680769:\n",
      "TensorBoard log found: runs/numeric_ppo_eval__UgandaNumeric__64__1737680769/events.out.tfevents.1737680771.HSPH-Mauricios-MacBook-Pro.95248.0\n",
      "Files for run numeric_ppo_eval__UgandaNumeric__64__1737680879:\n",
      "TensorBoard log found: runs/numeric_ppo_eval__UgandaNumeric__64__1737680879/events.out.tfevents.1737680880.HSPH-Mauricios-MacBook-Pro.95968.0\n",
      "Files for run numeric_ppo_eval__UgandaNumeric__64__1737680895:\n",
      "TensorBoard log found: runs/numeric_ppo_eval__UgandaNumeric__64__1737680895/events.out.tfevents.1737680896.HSPH-Mauricios-MacBook-Pro.96213.0\n",
      "Files for run random_policy_eval__Uganda__64__1737698877:\n",
      "TensorBoard log found: runs/random_policy_eval__Uganda__64__1737698877/events.out.tfevents.1737698886.holy8a24301.rc.fas.harvard.edu.1837871.0\n",
      "Files for run sac_attention_Uganda__v3-nt__gpt-4o-mini-huit__42__1737700847:\n",
      "Files for run sac_attention_Uganda__v3__gpt-4o-mini-huit__42__1737700847:\n",
      "Files for run sac_attention_Uganda__v3__gpt-4o-mini-huit__123__1737700849:\n",
      "Files for run sac_attention_Uganda__v3-nt__gpt-4o-mini-huit__123__1737700849:\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v3__42__1737700850:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v3__42__1737700850/events.out.tfevents.1737700865.holy7c08302.rc.fas.harvard.edu.1504796.0\n",
      "Files for run eval_llm_Uganda__base_agent__gpt-4o-mini-huit__v3__42__1737700850:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__v3__42__1737700850/events.out.tfevents.1737700865.holy7c08302.rc.fas.harvard.edu.1504794.0\n",
      "Files for run eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__v3__42__1737700850:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__v3__42__1737700850/events.out.tfevents.1737700865.holy7c08302.rc.fas.harvard.edu.1504795.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v3__42__1737700850:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v3__42__1737700850/events.out.tfevents.1737700865.holy7c08302.rc.fas.harvard.edu.1504797.0\n",
      "Files for run sac_attention_Uganda__v3__gpt-4o-mini-huit__456__1737700854:\n",
      "Files for run sac_attention_Uganda__v3-nt__gpt-4o-mini-huit__456__1737700854:\n",
      "Files for run eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__v3__123__1737700855:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__v3__123__1737700855/events.out.tfevents.1737700870.holy7c08402.rc.fas.harvard.edu.2361078.0\n",
      "Files for run eval_llm_Uganda__base_agent__gpt-4o-mini-huit__v3__123__1737700855:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__v3__123__1737700855/events.out.tfevents.1737700870.holy7c08402.rc.fas.harvard.edu.2361077.0\n",
      "Files for run eval_llm_Uganda__base_agent__gpt-4o-mini-huit__v3__456__1737700855:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__v3__456__1737700855/events.out.tfevents.1737700870.holy7c08402.rc.fas.harvard.edu.2361079.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v3__456__1737700855:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v3__456__1737700855/events.out.tfevents.1737700870.holy7c08402.rc.fas.harvard.edu.2361083.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v3__123__1737700855:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v3__123__1737700855/events.out.tfevents.1737700870.holy7c08402.rc.fas.harvard.edu.2361082.0\n",
      "Files for run eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__v3__456__1737700855:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_agent__gpt-4o-mini-huit__v3__456__1737700855/events.out.tfevents.1737700870.holy7c08402.rc.fas.harvard.edu.2361081.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v3__456__1737700855:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v3__456__1737700855/events.out.tfevents.1737700870.holy7c08402.rc.fas.harvard.edu.2361084.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v3__123__1737700855:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v3__123__1737700855/events.out.tfevents.1737700870.holy7c08402.rc.fas.harvard.edu.2361080.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v4__42__1737732651:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v4__42__1737732651/events.out.tfevents.1737732663.holy8a26502.rc.fas.harvard.edu.1769008.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v4__42__1737732651:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v4__42__1737732651/events.out.tfevents.1737732663.holy8a26502.rc.fas.harvard.edu.1769005.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v4__456__1737732651:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v4__456__1737732651/events.out.tfevents.1737732663.holy8a26502.rc.fas.harvard.edu.1769009.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v4__123__1737732651:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__gpt-4o-mini-huit__v4__123__1737732651/events.out.tfevents.1737732663.holy8a26502.rc.fas.harvard.edu.1769007.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v4__123__1737732651:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v4__123__1737732651/events.out.tfevents.1737732663.holy8a26502.rc.fas.harvard.edu.1769002.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v4__456__1737732651:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v4__456__1737732651/events.out.tfevents.1737732663.holy8a26502.rc.fas.harvard.edu.1769006.0\n",
      "Files for run sac_attention_Uganda__v4__gpt-4o-mini-huit__456__1737733090:\n",
      "Files for run sac_attention_Uganda__v4__gpt-4o-mini-huit__456__1737733090:\n",
      "Files for run sac_attention_Uganda__v4__gpt-4o-mini-huit__42__1737733100:\n",
      "Files for run sac_attention_Uganda__v4__gpt-4o-mini-huit__42__1737733100:\n",
      "Files for run sac_attention_Uganda__v4__gpt-4o-mini-huit__123__1737733106:\n",
      "Files for run sac_attention_Uganda__v4__gpt-4o-mini-huit__123__1737733106:\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v5__42__1737742125:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v5__42__1737742125/events.out.tfevents.1737742134.holy7c04201.rc.fas.harvard.edu.274839.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v5__123__1737742154:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v5__123__1737742154/events.out.tfevents.1737742167.holy7c04204.rc.fas.harvard.edu.3592648.0\n",
      "Files for run sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__42__1737742154:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__42__1737742154/events.out.tfevents.1737742167.holy7c04503.rc.fas.harvard.edu.2645275.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__42__1737742154/events.out.tfevents.1737742167.holy7c04503.rc.fas.harvard.edu.2645277.0\n",
      "Files for run sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__42__1737742154:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__42__1737742154/events.out.tfevents.1737742167.holy7c04503.rc.fas.harvard.edu.2645275.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__42__1737742154/events.out.tfevents.1737742167.holy7c04503.rc.fas.harvard.edu.2645277.0\n",
      "Files for run sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__123__1737742154:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__123__1737742154/events.out.tfevents.1737742167.holy7c04503.rc.fas.harvard.edu.2645276.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__123__1737742154/events.out.tfevents.1737742167.holy7c04503.rc.fas.harvard.edu.2645278.0\n",
      "Files for run sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__123__1737742154:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__123__1737742154/events.out.tfevents.1737742167.holy7c04503.rc.fas.harvard.edu.2645276.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__123__1737742154/events.out.tfevents.1737742167.holy7c04503.rc.fas.harvard.edu.2645278.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v5__456__1737742160:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__gpt-4o-mini-huit__v5__456__1737742160/events.out.tfevents.1737742174.holy7c04211.rc.fas.harvard.edu.3487040.0\n",
      "Files for run sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__456__1737742166:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__456__1737742166/events.out.tfevents.1737742182.holy7c04504.rc.fas.harvard.edu.3300750.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__456__1737742166/events.out.tfevents.1737742182.holy7c04504.rc.fas.harvard.edu.3300751.0\n",
      "Files for run sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__456__1737742166:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__456__1737742166/events.out.tfevents.1737742182.holy7c04504.rc.fas.harvard.edu.3300750.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta__meta.llama3-2-11b-instruct-v1:0__456__1737742166/events.out.tfevents.1737742182.holy7c04504.rc.fas.harvard.edu.3300751.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__meta.llama3-2-11b-instruct-v1:0__meta__42__1737745975:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__meta.llama3-2-11b-instruct-v1:0__meta__42__1737745975/events.out.tfevents.1737745982.holy7c04405.rc.fas.harvard.edu.1814009.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__meta.llama3-2-11b-instruct-v1:0__meta__42__1737745975:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__meta.llama3-2-11b-instruct-v1:0__meta__42__1737745975/events.out.tfevents.1737745982.holy7c04405.rc.fas.harvard.edu.1814010.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__meta.llama3-2-11b-instruct-v1:0__meta__123__1737745976:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__meta.llama3-2-11b-instruct-v1:0__meta__123__1737745976/events.out.tfevents.1737745982.holy7c04411.rc.fas.harvard.edu.2052561.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__meta.llama3-2-11b-instruct-v1:0__meta__123__1737745976:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__meta.llama3-2-11b-instruct-v1:0__meta__123__1737745976/events.out.tfevents.1737745982.holy7c04411.rc.fas.harvard.edu.2052560.0\n",
      "Files for run eval_llm_Uganda__no_thoughts_agent__meta.llama3-2-11b-instruct-v1:0__meta__456__1737746013:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__no_thoughts_agent__meta.llama3-2-11b-instruct-v1:0__meta__456__1737746013/events.out.tfevents.1737746027.holy7c04505.rc.fas.harvard.edu.1554332.0\n",
      "Files for run eval_llm_Uganda__llm_rules_no_thoughts__meta.llama3-2-11b-instruct-v1:0__meta__456__1737746013:\n",
      "TensorBoard log found: runs/eval_llm_Uganda__llm_rules_no_thoughts__meta.llama3-2-11b-instruct-v1:0__meta__456__1737746013/events.out.tfevents.1737746027.holy7c04505.rc.fas.harvard.edu.1554331.0\n",
      "Files for run sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__123__1737759486:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__123__1737759486/events.out.tfevents.1737759494.holy7c08507.rc.fas.harvard.edu.2464255.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__123__1737759486/events.out.tfevents.1737759494.holy7c08507.rc.fas.harvard.edu.2464256.0\n",
      "Files for run sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__123__1737759486:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__123__1737759486/events.out.tfevents.1737759494.holy7c08507.rc.fas.harvard.edu.2464255.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__123__1737759486/events.out.tfevents.1737759494.holy7c08507.rc.fas.harvard.edu.2464256.0\n",
      "Files for run sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__42__1737759534:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__42__1737759534/events.out.tfevents.1737759550.holy7c06311.rc.fas.harvard.edu.524280.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__42__1737759534/events.out.tfevents.1737759550.holy7c06311.rc.fas.harvard.edu.524281.0\n",
      "Files for run sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__42__1737759534:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__42__1737759534/events.out.tfevents.1737759550.holy7c06311.rc.fas.harvard.edu.524280.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__42__1737759534/events.out.tfevents.1737759550.holy7c06311.rc.fas.harvard.edu.524281.0\n",
      "Files for run sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__456__1737759737:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__456__1737759737/events.out.tfevents.1737759750.holy7c06506.rc.fas.harvard.edu.1390541.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__456__1737759737/events.out.tfevents.1737759750.holy7c06506.rc.fas.harvard.edu.1390542.0\n",
      "Files for run sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__456__1737759737:\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__456__1737759737/events.out.tfevents.1737759750.holy7c06506.rc.fas.harvard.edu.1390541.0\n",
      "TensorBoard log found: runs/sac_attention_Uganda__meta3b__meta.llama3-2-3b-instruct-v1:0__456__1737759737/events.out.tfevents.1737759750.holy7c06506.rc.fas.harvard.edu.1390542.0\n"
     ]
    }
   ],
   "source": [
    "# Use wandb.Api() to fetch runs\n",
    "import wandb\n",
    "\n",
    "# Initialize the API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Replace with your project and run details\n",
    "entity = \"rulebots\"\n",
    "project = \"rulebots\"\n",
    "\n",
    "# Fetch all runs\n",
    "runs = api.runs(f\"{entity}/{project}\")\n",
    "\n",
    "# Check for TensorBoard logs in workspace\n",
    "for run in runs:\n",
    "    files = run.files()\n",
    "    print(f\"Files for run {run.name}:\")\n",
    "    for file in files:\n",
    "        if \"events.out.tfevents\" in file.name or \"logs\" in file.name:\n",
    "            print(f\"TensorBoard log found: {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "b'runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__42__1737583030/events.out.tfevents.1737583044.holy7c08309.rc.fas.harvard.edu.1298129.0' does not point to valid Events file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m log_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__42__1737583030/events.out.tfevents.1737583044.holy7c08309.rc.fas.harvard.edu.1298129.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the event accumulator\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m event_acc \u001b[38;5;241m=\u001b[39m \u001b[43mEventAccumulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m event_acc\u001b[38;5;241m.\u001b[39mReload()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# List available tags in the TensorBoard file\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rulebots/lib/python3.10/site-packages/tensorboard/backend/event_processing/event_accumulator.py:312\u001b[0m, in \u001b[0;36mEventAccumulator.__init__\u001b[0;34m(self, path, size_guidance, compression_bps, purge_orphaned_data)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator_mutex \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator \u001b[38;5;241m=\u001b[39m \u001b[43m_GeneratorFromPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression_bps \u001b[38;5;241m=\u001b[39m compression_bps\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpurge_orphaned_data \u001b[38;5;241m=\u001b[39m purge_orphaned_data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rulebots/lib/python3.10/site-packages/tensorboard/backend/event_processing/event_accumulator.py:945\u001b[0m, in \u001b[0;36m_GeneratorFromPath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath must be a valid string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m io_wrapper\u001b[38;5;241m.\u001b[39mIsSummaryEventsFile(path):\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevent_file_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLegacyEventFileLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m directory_watcher\u001b[38;5;241m.\u001b[39mDirectoryWatcher(\n\u001b[1;32m    948\u001b[0m         path,\n\u001b[1;32m    949\u001b[0m         event_file_loader\u001b[38;5;241m.\u001b[39mLegacyEventFileLoader,\n\u001b[1;32m    950\u001b[0m         io_wrapper\u001b[38;5;241m.\u001b[39mIsSummaryEventsFile,\n\u001b[1;32m    951\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rulebots/lib/python3.10/site-packages/tensorboard/backend/event_processing/event_file_loader.py:136\u001b[0m, in \u001b[0;36mRawEventFileLoader.__init__\u001b[0;34m(self, file_path, detect_file_replacement)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_file_replacement \u001b[38;5;241m=\u001b[39m detect_file_replacement\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[43m_make_tf_record_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_file_replacement \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreopen\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile replacement detection requested, but not enabled because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF record iterator impl does not support reopening. This \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality requires TensorFlow 2.9+\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rulebots/lib/python3.10/site-packages/tensorboard/backend/event_processing/event_file_loader.py:59\u001b[0m, in \u001b[0;36m_make_tf_record_iterator\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__version__ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstub\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# TODO(#1711): Reshape stub implementation to fit tf_record_iterator API\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# rather than needlessly emulating the old PyRecordReader_New API.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpening a stub record reader pointing at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, file_path)\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_PyRecordReaderIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpywrap_tensorflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyRecordReader_New\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# If PyRecordReader exists, use it, otherwise use tf_record_iterator().\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Check old first, then new, since tf_record_iterator existed previously but\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# only gained the semantics we need at the time PyRecordReader was removed.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# support for TF 2.1 and prior, and find a non-deprecated replacement for\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# tf.compat.v1.io.tf_record_iterator.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rulebots/lib/python3.10/site-packages/tensorboard/backend/event_processing/event_file_loader.py:96\u001b[0m, in \u001b[0;36m_PyRecordReaderIterator.__init__\u001b[0;34m(self, py_record_reader_new, file_path)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a _PyRecordReaderIterator for the given file path.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m  py_record_reader_new: pywrap_tensorflow.PyRecordReader_New\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m  file_path: file path of the tfrecord file to read\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mraise_exception_on_not_ok_status() \u001b[38;5;28;01mas\u001b[39;00m status:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mpy_record_reader_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatus\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to open a record reader pointing to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m file_path\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rulebots/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/pywrap_tensorflow.py:177\u001b[0m, in \u001b[0;36mPyRecordReader_New.__init__\u001b[0;34m(self, filename, start_offset, compression_type, status)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo filename provided, cannot read Events\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gfile\u001b[38;5;241m.\u001b[39mexists(filename):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m does not point to valid Events file\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(filename),\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_offset:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnimplementedError(\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart offset not supported by compat reader\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: b'runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__42__1737583030/events.out.tfevents.1737583044.holy7c08309.rc.fas.harvard.edu.1298129.0' does not point to valid Events file"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# Path to your TensorBoard log file\n",
    "log_file_path = \"runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__42__1737583030/events.out.tfevents.1737583044.holy7c08309.rc.fas.harvard.edu.1298129.0\"\n",
    "\n",
    "# Load the event accumulator\n",
    "event_acc = EventAccumulator(log_file_path)\n",
    "event_acc.Reload()\n",
    "\n",
    "# List available tags in the TensorBoard file\n",
    "tags = event_acc.Tags()\n",
    "print(\"Available tags in the TensorBoard log:\")\n",
    "print(tags)\n",
    "\n",
    "# Extract scalar summaries for a specific tag\n",
    "# Replace 'scalars' with the desired tag name\n",
    "if \"scalars\" in tags:\n",
    "    scalar_data = event_acc.Scalars(\"scalars\")\n",
    "    print(\"\\nExtracted scalar data:\")\n",
    "    for event in scalar_data:\n",
    "        print(f\"Step: {event.step}, Value: {event.value}, Wall time: {event.wall_time}\")\n",
    "\n",
    "\n",
    "# Path to your TensorBoard log file\n",
    "log_file_path = \"runs/eval_llm_Uganda__base_agent__gpt-4o-mini-huit__eval_llm__42__1737583030/events.out.tfevents.1737583044.holy7c08309.rc.fas.harvard.edu.1298129.0\"\n",
    "\n",
    "# Load the event accumulator\n",
    "event_acc = EventAccumulator(log_file_path)\n",
    "event_acc.Reload()\n",
    "\n",
    "# List available tags in the TensorBoard file\n",
    "tags = event_acc.Tags()\n",
    "print(\"Available tags in the TensorBoard log:\")\n",
    "print(tags)\n",
    "\n",
    "# Extract scalar summaries for a specific tag\n",
    "# Replace 'scalars' with the desired tag name\n",
    "if \"scalars\" in tags:\n",
    "    scalar_data = event_acc.Scalars(\"scalars\")\n",
    "    print(\"\\nExtracted scalar data:\")\n",
    "    for event in scalar_data:\n",
    "        print(f\"Step: {event.step}, Value: {event.value}, Wall time: {event.wall_time}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
