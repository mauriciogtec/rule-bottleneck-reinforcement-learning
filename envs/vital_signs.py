import math
from typing import List, Optional

import numpy as np
from gymnasium import Env, spaces
from sklearn.mixture import GaussianMixture

from envs.wrappers import LanguageWrapper


def temperature_penalty(temperature):
    if temperature <= 38:
        return 0
    else:
        return -math.exp(abs(temperature - 38.0) / 2)  # Exponential penalty


def pulse_penalty(pulse):
    if pulse <= 120:
        return 0
    else:
        return -math.exp(abs(pulse - 120) / 17)  # Exponential penalty


def respiratory_penalty(respiratory_rate):
    if respiratory_rate <= 30:
        return 0
    else:
        return -math.exp(abs(respiratory_rate - 30) / 5)  # Exponential penalty


def spo2_penalty(spo2):
    # if 90 <= spo2: # HUGE BUG!!!
    if spo2 >= 90:
        return 0
    else:
        return -math.exp(abs(spo2 - 90) / 4)  # Exponential penalty


def blood_penalty(blood_pressure):
    if blood_pressure <= 127:
        return 0
    else:
        return -math.exp(abs(blood_pressure - 127) / 5)  # Exponential penalty


def reward_function(sign_dict, min_max, clip_value=1, scaler=0.1):
    reward = 0
    sign_dict = {
        sign: c * (min_max[sign][1] - min_max[sign][0]) + min_max[sign][0]
        for sign, c in sign_dict.items()
    }
    for signs in sign_dict:
        if signs == "COVERED_SKIN_TEMPERATURE":
            reward += temperature_penalty(sign_dict[signs])
        elif signs == "PULSE_RATE":
            reward += pulse_penalty(sign_dict[signs])
        elif signs == "RESPIRATORY_RATE":
            reward += respiratory_penalty(sign_dict[signs])
        elif signs == "SPO2":
            reward += spo2_penalty(sign_dict[signs])

    # Scale and clip the reward to avoid large values
    reward = reward * scaler
    reward = np.maximum(-clip_value, reward)

    return reward


class VitalSignsSimple(Env):

    def __init__(
        self,
        path: str,
        init_agents=4,  # B=3 in the paper
        # max_num_agents=10,  # N=20 in the paper
        budget=5,  # They have a budget, which does not necessarily eexample_rulesl to init_agent
        # t_min=1,  # t_min = 3 in the paper
        # t_max=5,  # t_max = 5 in the paper
        system_duration=10,  # = 50 in the paper, no letter
        degree_of_arm_noise=0.15,
        intervention_success_rate=0.7,
        variability_window=5,
        # joining_number=2,  # Here, vital signs only advance after N patients join
        # joining_interval=5,  # Here, simulate the number of internal vital signs steps
        T: Optional[int] = None,  # planning length / for finite horizon evaluation
        time_discount: Optional[float] = 0.99,  # discount factor for time,
        ignore_free_penalty: Optional[float] = 1.0,
    ):
        ## Check inputs
        assert (
            init_agents <= budget
        ), "The number of agents should be less than or equal to the budget"

        ## Random number generator
        self.np_random = np.random.default_rng()
        self.T = T
        self.ignore_free_penalty = ignore_free_penalty

        # load GMM
        self._load_gmm(path)

        ## We are in a finite time horizon
        # self.T = T
        # self.remaining_planning_length = T
        ## Number of agents at the initial timestep
        self.init_agents = init_agents
        self.num_agents = init_agents
        self.budget = budget
        # self.t_min = t_min
        # self.t_max = t_max
        # self.joining_number = joining_number
        self.system_duration = system_duration
        # self.joining_interval = joining_interval

        self.degree_of_arm_noise = degree_of_arm_noise
        self.intervention_success_rate = intervention_success_rate
        self.variability_window = variability_window

        ## Actions is a list of index that denotes agents who are pulled
        # total_action = math.comb(max_num_agents, budget)
        self.action_space = spaces.Discrete(budget)

        # The space of each device needs:
        self.nv = len(self.vital_signs)
        # 1. how many steps the current holder has worn the device (1)
        # 2. the sign history of the current holder (variability_window * nv)
        # 2. vital signs mean and std. dev. of the current holder (2 * nv)

        self.per_device_dim = 1 + (self.variability_window + 2) * self.nv
        self.total_dim = self.budget * self.per_device_dim
        # self.observation_space = spaces.Box(0, 1.0, shape=(self.total_dim,))
        self.observation_space = spaces.Box(
            0, 1.0, shape=(self.budget, self.per_device_dim), dtype=np.float32
        )

        ## Track agent states
        self.device_states = [{} for _ in range(self.budget)]

        # Time discount for future reward computation.
        self.time_discount = time_discount

    def _initialize_agents(self, init_agents=None):
        """Initialize the agents' states at timestep 0"""
        assignments = set(np.random.choice(self.budget, init_agents, replace=False))
        for i in range(self.budget):
            if i in assignments:
                state, _, mean, cov = self._sample_agent()
                current_vital, variability, signs_history = state

                # Update the device states
                self.device_states[i]["time_worn"] = 1
                self.device_states[i]["vitals"] = current_vital
                self.device_states[i]["variability"] = variability
                self.device_states[i]["signs_history"] = signs_history
                self.device_states[i]["gmm"] = mean, cov
            else:
                self.device_states[i]["time_worn"] = 0
                self.device_states[i]["vitals"] = np.zeros(self.nv)
                self.device_states[i]["variability"] = np.zeros(self.nv)
                sign_history = np.zeros((self.nv, self.variability_window))
                self.device_states[i]["signs_history"] = sign_history

    def _state_to_obs(self):
        # Initialize an empty list to hold rows
        agent_matrix = np.zeros((self.budget, self.per_device_dim), dtype=np.float32)

        # Iterate over each agent's state
        for j, agent in enumerate(self.device_states):
            # Extract values from the dictionary
            time_worn = agent["time_worn"] / self.system_duration

            # vitals = list(agent["vitals"])  # nv elements, ignore, part of history
            signs_history = np.array(agent["signs_history"], dtype=np.float32)
            mean, std = signs_history.mean(axis=1), signs_history.std(axis=1)

            # Concatenate all components into a single row
            agent_matrix[j, :] = np.concatenate(
                [[time_worn], signs_history.flatten(), mean, std]
            )

        return agent_matrix  # .flatten()

    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):
        super().reset(seed=seed)

        # Reset time counter
        self.t = 0

        # Empty the device states

        # Initialize agents at timestep 0
        options = options if options is not None else {}
        init_agents = options.get("init_agents", self.init_agents)
        self._initialize_agents(init_agents=init_agents)

        # # Set the remaining planning length
        # self.remaining_planning_length = self.T

        obs = self._state_to_obs()
        info = {}

        return obs, info

    def step(self, action: int):
        """

        Args:
            action (int): This is the device that we want to assign to a new patient
        """
        self.t += 1
        # 1. Assign the device, if the device has a current holder. If it does, compute the
        #    remaining reward/cost by simulating the rest of their time in the system.
        #    if the device is free, no reward, just assign the device to the new patient
        # 2. Update the device/agent states

        # == 1. Device assignment ==

        # Set the remaining planning length
        # self.remaining_planning_length -= 1

        # This will be an infinite horizon problem, and we will let gym
        # handle the time limit, which will set truncated to True when max steps reached
        terminated = (self.t >= self.T) if self.T else False
        truncated = False

        # Sample the new patient
        state, _, mean, cov = self._sample_agent()
        current_vital, variability, signs_history = state

        is_free = self.device_states[action]["time_worn"] == 0
        reward = 0.0

        gamma = self.time_discount

        num_free = sum([self.device_states[i]["time_worn"] == 0 for i in range(self.budget)])
        if not is_free:
            # first check that there was no remaining free device, otherwise penalzie
            if num_free > 0:
                reward -= self.ignore_free_penalty

            # simulate the rest of the time for the current holder
            mean, cov = self.device_states[action]["gmm"]
            time_worn = self.device_states[action]["time_worn"]

            if self.T is not None:
                remaining = min(self.T - self.t, self.system_duration - time_worn)
            else:
                remaining = self.system_duration - time_worn

            for t in range(remaining):
                # device is free, simulate remaining time without device
                state, r = self._simulate_one_step(action, intervention=False)

                # update agent state
                self.device_states[action]["time_worn"] += 1
                self.device_states[action]["vitals"] = current_vital
                self.device_states[action]["variability"] = variability
                self.device_states[action]["signs_history"] = signs_history

                reward += r * (gamma**t)

        # Assign the device to the new patient
        self.device_states[action]["time_worn"] = 0
        self.device_states[action]["vitals"] = current_vital
        self.device_states[action]["variability"] = variability
        self.device_states[action]["signs_history"] = signs_history
        self.device_states[action]["gmm"] = mean, cov

        # == 2. Update the remaining device/agents ==
        for i in range(self.budget):
            is_free = self.device_states[i]["time_worn"] == 0
            if not is_free or i == action:  # also compute for the new patient
                self.device_states[i]["time_worn"] += 1

                # advance the vital signs
                state, r = self._simulate_one_step(i, intervention=True)

                # update agent state
                self.device_states[i]["vitals"] = state[0]
                self.device_states[i]["variability"] = state[1]
                self.device_states[i]["signs_history"] = state[2]

                reward += r

                # remove them from the system if system reached
                if self.device_states[i]["time_worn"] >= self.system_duration:
                    self.device_states[i]["time_worn"] = 0
                    self.device_states[i]["vitals"] = np.zeros(self.nv)
                    self.device_states[i]["variability"] = np.zeros(self.nv)
                    sign_history = np.zeros((self.nv, self.variability_window))
                    self.device_states[i]["signs_history"] = sign_history

        obs = self._state_to_obs()

        if terminated:
            # for all devices compute the reward for the remaining time
            for i in range(self.budget):
                is_free = self.device_states[i]["time_worn"] == 0
                if not is_free:
                    mean, cov = self.device_states[i]["gmm"]
                    time_worn = self.device_states[i]["time_worn"]
                    gamma = self.time_discount
                    for t in range(time_worn, self.system_duration):
                        # device is free, simulate remaining time without device
                        state, r = self._simulate_one_step(i, intervention=True)

                        # update agent state
                        self.device_states[i]["time_worn"] += 1
                        self.device_states[i]["vitals"] = state[0]
                        self.device_states[i]["variability"] = state[1]
                        self.device_states[i]["signs_history"] = state[2]

                        reward += r * (gamma ** (t - time_worn))

        return obs, reward, terminated, truncated, {}

    def _conditional_sample_mnd(self, vital_values, mean, cov):
        """
        Sample from the conditional distribution of a multivariate Normal Distribution

        mean and cov were trained on trainsition pairs
        (st ; st+1) ~ MVN (m, S)
        then
        st+1 | st ~ MVN (m1, S1)
        m1 = m2 + S21 S-1 (st - m1)
        S1 = S22 - S21 S-1 S12
        """
        given_indices = np.arange(len(vital_values))
        remaining_indices = np.arange(len(vital_values), len(mean))

        # Calculate conditional means and covariances for each component
        mean_given = mean[given_indices]
        mean_remaining = mean[remaining_indices]
        cov_given_given = cov[np.ix_(given_indices, given_indices)]
        cov_remaining_given = cov[np.ix_(remaining_indices, given_indices)]
        cov_given_remaining = cov[np.ix_(given_indices, remaining_indices)]
        cov_remaining_remaining = cov[np.ix_(remaining_indices, remaining_indices)]
        # print("means",mean_given,mean_remaining)
        # print("covariates",cov_given_given,cov_remaining_given,cov_given_remaining,cov_remaining_remaining)

        cov_inv_given_given = np.linalg.inv(cov_given_given)
        conditional_mean = (
            mean_remaining
            + cov_remaining_given @ cov_inv_given_given @ (vital_values - mean_given)
        )
        conditional_cov = (
            cov_remaining_remaining
            - cov_remaining_given @ cov_inv_given_given @ cov_given_remaining
        )

        v = self.np_random.multivariate_normal(
            mean=conditional_mean, cov=conditional_cov
        )

        return np.clip(v, 0, 1)

    # current_values, min_max, intervention_success_rate, mean=None, cov=None,
    def _interventions(self, vital_values, mean, cov):
        """interventions: This function models the effect of intervention. If the patient's value
        falls in the normal range, then the patient's next state will be sampled from a multivariate
        Guassian from this current state

        If the patient's vital sign shows abnormality, then there is a 30% chance the doctors do not
        intervene, and there is a 70% chance the intervention creates a positive effect on the patient.
        After applying the positive effect, the patient's new state will be the condition for sampling
        the next state
        """
        vital_signs = self.vital_signs
        min_max = self.min_max

        rew = reward_function(dict(zip(vital_signs, vital_values)), min_max)
        if rew >= 0:
            return self._conditional_sample_mnd(vital_values, mean, cov)
        else:
            # new_signs= conditional_sample_gmm(gmm, current_values, given_indices,component_index=component_index)
            # print("Old", current_values)
            new_signs = self._improve_vital_signs3(
                dict(zip(vital_signs, vital_values)), mean, cov
            )
            # print("NEW",[new_signs[vital] for vital in vital_signs])
            return self._conditional_sample_mnd(
                [new_signs[vital] for vital in vital_signs], mean, cov
            )

    def _simulate_one_step(self, device_index, intervention=False):
        """simulate_one_step: based on the current value, calculate what's the next state for vital signs,
        the variance of vital sign for the past five timesteps, and the reward
        """

        vital_signs = self.vital_signs
        min_max = self.min_max

        mean, cov = self.device_states[device_index]["gmm"]
        vitals = self.device_states[device_index]["vitals"]
        signs_history = self.device_states[device_index]["signs_history"]
        variability = self.device_states[device_index]["variability"]

        if intervention:
            next_signs = self._interventions(vitals, mean, cov)
        else:
            next_signs = self._conditional_sample_mnd(vitals, mean, cov)

        for i in range(len(vital_signs)):
            signs_history[i].pop(0)
            signs_history[i].append(next_signs[i])

        # Note: Mauricio changed to standard deviation, better normalization
        variability = np.array([np.std(l) for l in signs_history])

        reward = reward_function(dict(zip(vital_signs, next_signs)), min_max)
        return [next_signs, variability, signs_history], reward

    def _load_gmm(self, path: str) -> GaussianMixture:
        data = np.load(path)
        means = np.array(data["means"])
        covariances = np.array(data["covariances"])
        weights = np.array(data["weights"])
        scaler_min = np.array(data["scaler_min"])
        scaler_max = np.array(data["scaler_max"])
        names = list((data["names"]))

        min_max = {
            name: [min_val, max_val]
            for name, min_val, max_val in zip(names, scaler_min, scaler_max)
        }

        # Reconstruct the GMM
        gmm = GaussianMixture(n_components=len(weights), covariance_type="full")

        # Manually set the parameters
        gmm.weights_ = weights
        gmm.means_ = means
        gmm.covariances_ = covariances

        # Compute the precisions_cholesky_ required by the GaussianMixture object
        gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(covariances))

        self.gmm = gmm
        self.min_max = min_max
        self.vital_signs = names
        self.given_indices = np.arange(len(self.vital_signs))

    def _sample_agent(self):  # moves this to in class, all functions should be in class
        """sample_agent: you choose a component basesd on weight of each component for the multivariate
        Gaussian, then you get the sample from it.
        You perturb the vital sign mean and cov by choosing a mean and covariance from another component
        in the mixture model, and randomly sampling a influence factor to determine the magnitude of
        pertubation
        """
        gmm = self.gmm
        self.min_max = self.min_max

        weights = gmm.weights_

        # Normalize the weights to ensure they sum to 1
        weights /= np.sum(weights)

        # Sample an index based on the weights
        component = self.np_random.choice(len(weights), p=weights)

        means = gmm.means_
        covariances = gmm.covariances_
        mean = means[component]
        cov = covariances[component]
        state, _ = self._resample_values(mean, cov)

        perturb = self.np_random.choice(
            [i for i in range(len(weights)) if i != component]
        )

        x = self.np_random.uniform(0, self.degree_of_arm_noise)
        y = self.np_random.uniform(0, self.degree_of_arm_noise)

        mean = (1 - x) * mean + x * means[perturb]
        cov = (1 - y) * cov + y * covariances[perturb]

        # print(mean,cov)
        # pertubation
        return state, component, mean, cov

    def _resample_values(self, mean, cov):
        """resample_values: You sample from a multivariate Gaussian for your initial value,
        and you sample conditioned on the previous value until you have enough sign history to
        calculate variability

        Then you return the current signs, the variability of the past timesteps, the past
        vital sign values, and the corresponding reward of the currrent vital sign
        """
        variability_window = self.variability_window

        vital_signs = self.vital_signs
        min_max = self.min_max
        given_indices = np.arange(len(vital_signs))

        sample = self.np_random.multivariate_normal(mean=mean, cov=cov)
        sample = np.clip(sample, 0, 1)

        current_signs = [sample[i] for i in given_indices]
        signs_history = [[] for _ in range(len(vital_signs))]
        for i in range(len(vital_signs)):
            signs_history[i].append(sample[i])

        for _ in range(variability_window - 1):
            current_signs = self._conditional_sample_mnd(current_signs, mean, cov)
            for i in range(len(vital_signs)):
                signs_history[i].append(current_signs[i])

        # print(signs_history)
        # for l in signs_history:
        # print(l,np.var(l))
        variability = np.array([np.std(l) for l in signs_history])
        # print(variability)
        reward = reward_function(dict(zip(vital_signs, current_signs)), min_max)
        return [current_signs, variability, signs_history], reward

    # sign_dict, min_max, intervention_success_rate
    def _improve_vital_signs3(self, sign_dict, mean, cov):
        """improve_vital_signs: Another model of the positive effect of intervention
        (assigning a medical device). The medical staff reacts to the alert 70%
        of time in reality, and the abnormal vital sign is adjusted probablistically
        towards the normal. This seems to be the model used in the final paper.
        """

        min_max = self.min_max
        if min_max:
            # normalize
            sign_dict = {
                sign: c * (min_max[sign][1] - min_max[sign][0]) + min_max[sign][0]
                for sign, c in sign_dict.items()
            }

        # print(sign_dict)
        if self.np_random.random() < self.intervention_success_rate:
            for signs in sign_dict:
                if signs == "COVERED_SKIN_TEMPERATURE":
                    if temperature_penalty(sign_dict[signs]) < 0:
                        delta = self.np_random.normal(1.5, 0.5)
                        sign_dict[signs] -= delta
                elif signs == "PULSE_RATE":
                    if pulse_penalty(sign_dict[signs]) < 0:
                        delta = self.np_random.normal(15, 5)
                        sign_dict[signs] -= delta
                elif signs == "RESPIRATORY_RATE":
                    if respiratory_penalty(sign_dict[signs]) < 0:
                        delta = self.np_random.normal(10, 10 / 3)
                        sign_dict[signs] -= delta
                elif signs == "SPO2":
                    if spo2_penalty(sign_dict[signs]) < 0:
                        delta = self.np_random.normal(5, 5 / 3)
                        sign_dict[signs] += delta

        if min_max:
            # renormalize
            sign_dict = {
                sign: (c - min_max[sign][0]) / (min_max[sign][1] - min_max[sign][0])
                for sign, c in sign_dict.items()
            }

        return sign_dict


class VitalSignsSimpleLang(LanguageWrapper):
    def __init__(self, path: str, **kwargs):
        env = VitalSignsSimple(path, **kwargs)
        super().__init__(env)

    _state_mapping = {
        "PULSE_RATE": "Pulse rate",
        "RESPIRATORY_RATE": "Respiratory rate",
        "SPO2": "SPO2",
        "COVERED_SKIN_TEMPERATURE": "Covered skin temperature",
    }

    @property
    def task_text(self) -> str:
        return (
            "You are assisting doctors from a hospital in making optimized"
            " decisions about which patient should receive a vital sign monitor device."
            " The device can help improve the patient's vital signs. "
            " Your goal is to ensure the optimal allocation of devices, such that patients with a higher"
            " risk continue wearing a device until their vital signs are within the normal range. Since"
            " there is a limited number of devices, you will need to decide which patients should stop"
            " wearing the device to reallocate it to the incoming patients. Incoming patients must"
            " always receive a device.\n"
            "Cost Function: A cost will be inccoured if the pulse rate exceeds 120, the temperature exceeds 38C, the respiratory rate exceeds 30,"
            " or if the SPO2 rate falls below 90. The cost is calculated as an exponential function of the deviation from these thresholds.\n"
            "Effect of Intervention: The abnormal vital signs of patients wearing a device are reduced towards their normal range with an estimated"
            " 70% success rate. "
        )

    @property
    def action_space_text(self) -> str:
        return (
            # "Choose the id of the device that will be reallocated to the new incoming patient."
            "Which device do you pick?"
            f" Your answer should be a single integer i from 0 to {self.env.budget} (the number of devices)."
            # "- Always choose a free device if available\n"
            # "- If no free device is available, then choose device i whose current patient is at least risk or"
            # " would benefit less from wearing the device."
            " Format your answer as a JSON as in the following examples: {'device': 0}, {'device': 3}."
        )

    def state_descriptor(self, *_, **__) -> str:
        env = self.env
        min_max = env.min_max
        win = env.variability_window

        lower = np.array([min_max[v][0] for v in env.vital_signs])
        upper = np.array([min_max[v][1] for v in env.vital_signs])

        desc = f"Number of devices: {env.budget}\n"

        is_free = [device["time_worn"] == 0 for device in env.device_states]
        num_free = sum(is_free)
        if num_free == 0:
            desc += "Number of free devices: none\n\n\n"
        else:
            desc += f"Number of free devices: {num_free}\n"
            free = [i for i, free in enumerate(is_free) if free]
            desc += f"Ids of free devices: {free}\n\n\n"

        desc_bits = []

        for i, device in enumerate(env.device_states):
            time_worn = device["time_worn"]

            s = f"### Device {i}\n\n"

            if is_free[i]:
                # device is free
                s += "Device is currently free.\n\n"
            else:
                # device is currently assigned
                s += "Device is currently assigned to a patient with the following description:\n\n"

                # unnormalize the values
                signs_history = np.array(device["signs_history"])
                signs_history *= (upper - lower)[:, None]
                signs_history += lower[:, None]
                mean = signs_history.mean(axis=1)
                std = signs_history.std(axis=1)

                s += f"*Timesteps wearing the device*: {time_worn}\n\n"
                for j, v in enumerate(env.vital_signs):
                    hist = ", ".join(f"{x:.2f}" for x in signs_history[j])
                    s += f"*{self._state_mapping[v]}*\n"
                    # s += f"- Recent history (oldest to newest): {hist}\n"
                    s += f"- Last value: {signs_history[j][-1]:.2f}\n"
                    s += f"- Mean: {mean[j]:.2f}\n"
                    s += f"- Standard deviation/volatility: {std[j]:.2f}\n\n"

            desc_bits.append(s)

        desc += "\n".join(desc_bits)

        return desc

    @property
    def example_rules(self) -> List[str]:
        rule_1 = (
            '{"background": "If a device is free, no patient is receiving the benefit.",'
            ' "rule": "If device i is free, assign it to the new patient.",'
            ' "state relevance": "Devices 0, 1 and 3 are currently free. Any of them are good actions."}'
        )

        rule_2 = (
            '{"background": "Patients with high volatility in their vital signs are at higher risk of abnormal vital signs even if their last observed signs are normal",'
            ' "rule": "When there are no free devices, reallocate the devices of patients with low volatility and normal vital signs."n'
            ' "state relevance": "No devices are free. Device #3 has a high blood pressure volatility (+- 30), so device #3 is not a good action."}'
        )

        rule_3 = (
            '{"background": "Patients with abnormal vital signs will benefit from continued use of the device",'
            ' "rule": "Prioritize reallocating the devices of patients with normal vital signs over those with abnormal vital signs",'
            ' "state relevance": "The patient wearing device #2 is at risk because it has low SPO2 (85%), so Device ID should *not* be reallocated. Patients wearing devices 0, 1, 3, 4 are not at risk, so they are candidate actions."}'

        )
        return [rule_1, rule_2, rule_3]


if __name__ == "__main__":
    # env = VitalSignsLang(path="models/uganda.npz", parse_action=True)

    # # task step
    # print(f"\n\n== Task Step ==\n{env.task_text}")

    # # action space text
    # print(f"\n\n== Action Space Text ==\n{env.action_space_text}")

    # # reset
    # obs, info = env.reset()
    # print(f"Initial state:\n {obs[1]}")

    # print(f"\n\n== Step: {0} == ")
    # ## One step
    # obs, reward, terminated, truncated, info = env.step("[]")
    # print(f"State:\n {obs[1]}")
    # print(f"Reward: {reward}")
    # print(f"Terminated: {terminated}")
    # print(f"Truncated: {truncated}")

    # print(f"\n\n== Step: {1} == ")
    # ## Four steps
    # obs, reward, terminated, truncated, info = env.step("[2, 3]")
    # print(f"State:\n {obs[1]}")
    # print(f"Reward: {reward}")
    # print(f"Terminated: {terminated}")
    # print(f"Truncated: {truncated}")

    # print(f"\n\n== Step: {2} == ")
    # ## One step
    # obs, reward, terminated, truncated, info = env.step("[]")
    # print(f"State:\n {obs[1]}")
    # print(f"Reward: {reward}")
    # print(f"Terminated: {terminated}")
    # print(f"Truncated: {truncated}")

    # print(f"\n\n== Step: {3} == ")
    # ## Four steps
    # obs, reward, terminated, truncated, info = env.step("[4, 5]")
    # print(f"State:\n {obs[1]}")
    # print(f"Reward: {reward}")
    # print(f"Terminated: {terminated}")
    # print(f"Truncated: {truncated}")

    # print(f"\n\n== Step: {4} == ")
    # ## One step
    # obs, reward, terminated, truncated, info = env.step("[]")
    # print(f"State:\n {obs[1]}")
    # print(f"Reward: {reward}")
    # print(f"Terminated: {terminated}")
    # print(f"Truncated: {truncated}")

    # for step in range(5):
    #     print(f"\n\n== Step: {step} == ")
    #     # action
    #     obs, reward, terminated, truncated, info = env.step("[0, 1]")
    #     print(f"State:\n {obs[1]}")
    #     print(f"Reward: {reward}")
    #     print(f"Terminated: {terminated}")
    #     print(f"Truncated: {truncated}")

    import envs as E
    import gymnasium as gym

    # env = VitalSignsSimpleLang(path="models/uganda.npz")
    env = gym.make("Uganda")

    # reset
    obs, _ = env.reset()

    print(f"Initial state:\n {obs}")

    for i in range(18):
        print(f"\n\n== Step: {i} == ")
        obs, reward, terminated, truncated, _ = env.step(0)
        print(obs[1])
        print(f"Reward: {reward}")

    0
